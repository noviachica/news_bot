import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import os
import json
import requests
from bs4 import BeautifulSoup
import base64
from datetime import datetime

print("1. ì‹œì‘...")

# âœ… STEP 1: Google Sheetsì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
try:
    print("2. Google ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ì½ê¸° ì‹œë„...")
    with open('google_credentials.json', 'r') as f:
        creds_json = json.load(f)
    print("3. Google ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ì½ê¸° ì„±ê³µ")

    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive.file",
        "https://www.googleapis.com/auth/drive"
    ]

    print("4. Google Sheets ì¸ì¦ ì‹œë„...")
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_json, scope)
    client = gspread.authorize(creds)
    print("5. Google Sheets ì¸ì¦ ì„±ê³µ")

    print("6. ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì ‘ê·¼ ì‹œë„...")
    sheet_url = 'https://docs.google.com/spreadsheets/d/1HLTb59lcJQIZmaPMrJ0--hEsheyERIkCg5aBxSEFDtc/edit#gid=0'
    sheet = client.open_by_url(sheet_url).worksheet("Result")
    data = sheet.get_all_records()
    df = pd.DataFrame(data)
    print("7. ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„° ë¡œë“œ ì„±ê³µ")
    print(f"8. ì´ {len(df)}ê°œì˜ ê¸°ì‚¬ê°€ ìˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"âŒ Google Sheets ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# âœ… STEP 2: ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ëŸ¬ ì •ì˜
def extract_article_text(url):
    try:
        print(f"\ní¬ë¡¤ë§ ì‹œë„: {url}")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()  # HTTP ì—ëŸ¬ ì²´í¬
        print(f"HTTP ìƒíƒœ ì½”ë“œ: {res.status_code}")
        
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì— ëŒ€í•œ ì„ íƒì
        selectors = [
            'div#dic_area',  # Naver ë‰´ìŠ¤
            'div.article-body',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'article',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'div.article-content',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'div.article-text',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'div#articleBody',  # ë¹„ì¦ˆë‹ˆìŠ¤í¬ìŠ¤íŠ¸
            'div#news_body_area',  # ë¹„ë°”100
            'div#article-view-content-div',  # ë‰´ìŠ¤í”„ë¼ì„
            'div.article_body',  # ë”œì‚¬ì´íŠ¸
            'div#articeBody',  # ë„¤ì´ë²„ ì—”í„°í…Œì¸ë¨¼íŠ¸
            'div.end_body',  # ë„¤ì´ë²„ ì—”í„°í…Œì¸ë¨¼íŠ¸ ëª¨ë°”ì¼
            'div#newsEndContents',  # ë„¤ì´ë²„ ìŠ¤í¬ì¸ 
            'div.article_txt',  # ì¶”ê°€ ì¼ë°˜
            'div#articleContent',  # ì¶”ê°€ ì¼ë°˜
            'div#newsContent',  # ì¶”ê°€ ì¼ë°˜
            'div.news_body',  # ì¶”ê°€ ì¼ë°˜
            'div#newsViewArea',  # ì¶”ê°€ ì¼ë°˜
            'div#content',  # ì¶”ê°€ ì¼ë°˜
        ]
        
        for selector in selectors:
            article = soup.select_one(selector)
            if article:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for tag in article.select('script, style, iframe, .reporter_area, .copyright, .promotion'):
                    tag.decompose()
                
                # í…ìŠ¤íŠ¸ ì •ì œ
                text = article.get_text(strip=True)
                text = ' '.join(text.split())  # ì—°ì†ëœ ê³µë°± ì œê±°
                
                print(f"ì„±ê³µì ìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì„ íƒì: {selector}")
                return text
        
        print("ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì„ íƒì:")
        for selector in selectors:
            elements = soup.select(selector)
            print(f"{selector}: {len(elements)}ê°œ ë°œê²¬")
            
        # ë§ˆì§€ë§‰ ì‹œë„: ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
        paragraphs = soup.find_all(['p', 'div'])
        if paragraphs:
            longest_text = max(paragraphs, key=lambda p: len(p.get_text(strip=True)))
            text = longest_text.get_text(strip=True)
            if len(text) > 100:  # ìµœì†Œ 100ì ì´ìƒì¸ ê²½ìš°ë§Œ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
                print("ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                return text
            
        return "ë³¸ë¬¸ ì—†ìŒ"

    except requests.exceptions.RequestException as e:
        print(f"ìš”ì²­ ì—ëŸ¬: {e}")
        return f"[í¬ë¡¤ë§ ì—ëŸ¬] ìš”ì²­ ì‹¤íŒ¨: {e}"
    except Exception as e:
        print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
        return f"[í¬ë¡¤ë§ ì—ëŸ¬] {e}"

# âœ… STEP 3: ê¸°ì‚¬ ë³¸ë¬¸ ì—´ ì¶”ê°€
print("\n9. ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘...")
try:
    df['ë³¸ë¬¸'] = df['ë§í¬'].apply(extract_article_text)
    print("10. ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# âœ… STEP 4: JSON ë³€í™˜
print("11. JSON ë³€í™˜ ì‹œì‘...")
try:
    json_content = df.to_json(orient='records', force_ascii=False, indent=2)
    print("12. JSON ë³€í™˜ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ JSON ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# âœ… STEP 5: GitHub Repository ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_github_repo(json_data, github_token, repo_name, file_path='news_batch.json'):
    try:
        print("13. GitHub Repository ì—…ë°ì´íŠ¸ ì‹œë„...")
        # GitHub API ì—”ë“œí¬ì¸íŠ¸
        url = f"https://api.github.com/repos/{repo_name}/contents/{file_path}"
        print(f"API URL: {url}")
        
        # ê¸°ì¡´ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        headers = {
            "Authorization": f"Bearer {github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        print("í—¤ë” ì„¤ì • ì™„ë£Œ")
        
        # ê¸°ì¡´ íŒŒì¼ì˜ SHA ê°€ì ¸ì˜¤ê¸°
        print("ê¸°ì¡´ íŒŒì¼ ì •ë³´ ìš”ì²­ ì¤‘...")
        response = requests.get(url, headers=headers)
        print(f"ê¸°ì¡´ íŒŒì¼ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        sha = None
        if response.status_code == 200:
            sha = response.json()['sha']
            print("ê¸°ì¡´ íŒŒì¼ SHA íšë“")
        else:
            print("ìƒˆ íŒŒì¼ ìƒì„±")
        
        # íŒŒì¼ ì—…ë°ì´íŠ¸
        payload = {
            "message": f"Update news data {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "content": base64.b64encode(json_data.encode()).decode(),
            "sha": sha
        }
        print("ì—…ë°ì´íŠ¸ ìš”ì²­ ì „ì†¡ ì¤‘...")
        
        response = requests.put(url, headers=headers, data=json.dumps(payload))
        print(f"ì—…ë°ì´íŠ¸ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        print(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
        
        if response.status_code in [200, 201]:
            raw_url = response.json()['content']['download_url']
            print(f"âœ… GitHub Repository ì—…ë°ì´íŠ¸ ì„±ê³µ!\nğŸŒ Raw URL: {raw_url}")
            return raw_url
        else:
            print(f"âŒ GitHub Repository ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {response.status_code}\n{response.text}")
            return None
    except Exception as e:
        print(f"âŒ GitHub Repository ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# âœ… STEP 6: GitHub Repositoryì— ì—…ë¡œë“œ
GITHUB_REPO = "noviachica/news_bot"  # GitHub ì €ì¥ì†Œ ì´ë¦„
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')  # GitHub Actionsì—ì„œ ì œê³µí•˜ëŠ” í† í° ì‚¬ìš©

if GITHUB_TOKEN:
    update_github_repo(json_content, GITHUB_TOKEN, GITHUB_REPO)
else:
    print("âš ï¸ GITHUB_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GitHub ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.") 
