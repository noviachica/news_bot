import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import os
import json
import requests
from bs4 import BeautifulSoup
import base64
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
from konlpy.tag import Okt  # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ìš©

print("1. ì‹œì‘...")

# âœ… STEP 1: Google Sheetsì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
def get_google_sheets_data():
    print("2. Google ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ì½ê¸° ì‹œë„...")
    
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ JSON ë¬¸ìì—´ì„ ì½ì–´ì„œ íŒŒì‹±
        credentials_json = os.environ.get('GOOGLE_CREDENTIALS')
        if not credentials_json:
            print("í™˜ê²½ ë³€ìˆ˜ ë‚´ìš©:")
            print(os.environ)
            raise ValueError("GOOGLE_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        print("3. JSON ë°ì´í„° ê¸¸ì´:", len(credentials_json))
        print("4. JSON ë°ì´í„° ì‹œì‘ ë¶€ë¶„:", credentials_json[:100])  # ì²˜ìŒ 100ìë§Œ ì¶œë ¥
        
        credentials_dict = json.loads(credentials_json)
        
        # ìŠ¤ì½”í”„ ì„¤ì •
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive.file",
            "https://www.googleapis.com/auth/drive"
        ]
        
        credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
        gc = gspread.authorize(credentials)
        print("6. Google Sheets ì¸ì¦ ì„±ê³µ")
        
        # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸°
        spreadsheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1HLTb59lcJQIZmaPMrJ0--hEsheyERIkCg5aBxSEFDtc/edit#gid=0')
        
        # Result ì›Œí¬ì‹œíŠ¸ ì„ íƒ
        worksheet = spreadsheet.worksheet("Result")
        print(f"7. ì„ íƒëœ ì›Œí¬ì‹œíŠ¸: {worksheet.title}")
        
        # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_values = worksheet.get_all_records()  # ë³€ê²½ëœ ë¶€ë¶„
        print(f"8. ì „ì²´ ë°ì´í„° í–‰ ìˆ˜: {len(all_values)}")
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(all_values)
        print(f"9. DataFrame ìƒì„± ì™„ë£Œ. í–‰ ìˆ˜: {len(df)}")
        print("10. ì»¬ëŸ¼:", df.columns.tolist())
        
        # ì‹ ë¬¸ì‚¬ ì •ë³´ ì¶”ì¶œ (URLì—ì„œ)
        def extract_newspaper(url):
            if 'naver.com' in url:
                # ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ ì‹ ë¬¸ì‚¬ ì½”ë“œ ì¶”ì¶œ
                newspaper_codes = {
                    '023': 'ì¡°ì„ ì¼ë³´',
                    '025': 'ì¤‘ì•™ì¼ë³´',
                    '020': 'ë™ì•„ì¼ë³´',
                    '032': 'ê²½í–¥ì‹ ë¬¸',
                    '028': 'í•œê²¨ë ˆì‹ ë¬¸',
                    '469': 'í•œêµ­ì¼ë³´',
                    '009': 'ë§¤ì¼ê²½ì œ',
                    '015': 'í•œêµ­ê²½ì œ',
                    '011': 'ì„œìš¸ê²½ì œ',
                    '277': 'ì•„ì£¼ê²½ì œ'
                }
                try:
                    code = url.split('/article/')[1].split('/')[0]
                    return newspaper_codes.get(code, 'ê¸°íƒ€')
                except:
                    return 'ê¸°íƒ€'
            return 'ê¸°íƒ€'
        
        # ì‹ ë¬¸ì‚¬ ì—´ ì¶”ê°€
        df['ì‹ ë¬¸ì‚¬'] = df['ë§í¬'].apply(extract_newspaper)
        print("11. ì‹ ë¬¸ì‚¬ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
        
        return df
        
    except Exception as e:
        print(f"âŒ Google Sheets ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(f"ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}")
        raise

# âœ… STEP 2: ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ëŸ¬ ì •ì˜
def extract_article_text(url):
    try:
        print(f"\ní¬ë¡¤ë§ ì‹œë„: {url}")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()  # HTTP ì—ëŸ¬ ì²´í¬
        print(f"HTTP ìƒíƒœ ì½”ë“œ: {res.status_code}")
        
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì— ëŒ€í•œ ì„ íƒì
        selectors = [
            'div#dic_area',  # Naver ë‰´ìŠ¤
            'div.article-body',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'article',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'div.article-content',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'div.article-text',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'div#articleBody',  # ë¹„ì¦ˆë‹ˆìŠ¤í¬ìŠ¤íŠ¸
            'div#news_body_area',  # ë¹„ë°”100
            'div#article-view-content-div',  # ë‰´ìŠ¤í”„ë¼ì„
            'div.article_body',  # ë”œì‚¬ì´íŠ¸
            'div#articeBody',  # ë„¤ì´ë²„ ì—”í„°í…Œì¸ë¨¼íŠ¸
            'div.end_body',  # ë„¤ì´ë²„ ì—”í„°í…Œì¸ë¨¼íŠ¸ ëª¨ë°”ì¼
            'div#newsEndContents',  # ë„¤ì´ë²„ ìŠ¤í¬ì¸ 
            'div.article_txt',  # ì¶”ê°€ ì¼ë°˜
            'div#articleContent',  # ì¶”ê°€ ì¼ë°˜
            'div#newsContent',  # ì¶”ê°€ ì¼ë°˜
            'div.news_body',  # ì¶”ê°€ ì¼ë°˜
            'div#newsViewArea',  # ì¶”ê°€ ì¼ë°˜
            'div#content',  # ì¶”ê°€ ì¼ë°˜
            'div.article',  # ë„¤ì´ë²„ ì—”í„°í…Œì¸ë¨¼íŠ¸ ì¶”ê°€
            'div.article-content',  # ë„¤ì´ë²„ ì—”í„°í…Œì¸ë¨¼íŠ¸ ì¶”ê°€
            'div.article-body',  # ë„¤ì´ë²„ ì—”í„°í…Œì¸ë¨¼íŠ¸ ì¶”ê°€
        ]
        
        for selector in selectors:
            article = soup.select_one(selector)
            if article:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for tag in article.select('script, style, iframe, .reporter_area, .copyright, .promotion'):
                    tag.decompose()
                
                # í…ìŠ¤íŠ¸ ì •ì œ
                text = article.get_text(strip=True)
                text = ' '.join(text.split())  # ì—°ì†ëœ ê³µë°± ì œê±°
                
                print(f"ì„±ê³µì ìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì„ íƒì: {selector}")
                return text
        
        print("ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì„ íƒì:")
        for selector in selectors:
            elements = soup.select(selector)
            print(f"{selector}: {len(elements)}ê°œ ë°œê²¬")
            
        # ë§ˆì§€ë§‰ ì‹œë„: ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
        paragraphs = soup.find_all(['p', 'div'])
        if paragraphs:
            longest_text = max(paragraphs, key=lambda p: len(p.get_text(strip=True)))
            text = longest_text.get_text(strip=True)
            if len(text) > 100:  # ìµœì†Œ 100ì ì´ìƒì¸ ê²½ìš°ë§Œ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
                print("ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                return text
            
        return "ë³¸ë¬¸ ì—†ìŒ"

    except requests.exceptions.RequestException as e:
        print(f"ìš”ì²­ ì—ëŸ¬: {e}")
        return f"[í¬ë¡¤ë§ ì—ëŸ¬] ìš”ì²­ ì‹¤íŒ¨: {e}"
    except Exception as e:
        print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
        return f"[í¬ë¡¤ë§ ì—ëŸ¬] {e}"

# ì‹ ë¬¸ì‚¬ ê·¸ë£¹ ì •ì˜
NEWSPAPER_GROUPS = {
    'ë³´ìˆ˜': ['ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´'],
    'ì§„ë³´': ['ê²½í–¥ì‹ ë¬¸', 'í•œê²¨ë ˆì‹ ë¬¸', 'í•œêµ­ì¼ë³´'],
    'ê²½ì œ': ['ë§¤ì¼ê²½ì œ', 'í•œêµ­ê²½ì œ', 'ì„œìš¸ê²½ì œ', 'ì•„ì£¼ê²½ì œ']
}

def get_newspaper_group(newspaper):
    """ì‹ ë¬¸ì‚¬ê°€ ì†í•œ ê·¸ë£¹ì„ ë°˜í™˜"""
    for group_name, newspapers in NEWSPAPER_GROUPS.items():
        if newspaper in newspapers:
            return group_name
    return 'ê¸°íƒ€'

def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    if not isinstance(text, str):
        return ""
        
    # í•œê¸€, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¸°ê¸°
    text = re.sub(r'[^ê°€-í£0-9\s]', ' ', text)
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def calculate_similarity(text1, text2):
    """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    if not text1 or not text2:
        return 0.0
        
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    text1 = preprocess_text(text1)
    text2 = preprocess_text(text2)
    
    vectorizer = TfidfVectorizer(
        max_features=10000,  # ìµœëŒ€ íŠ¹ì„± ìˆ˜ ì œí•œ
        min_df=2,  # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„
        max_df=0.95  # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return similarity
    except Exception as e:
        print(f"ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        return 0.0

def group_similar_articles(articles, similarity_threshold=0.3):
    """
    ìœ ì‚¬í•œ ê¸°ì‚¬ë¥¼ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’ 0.3)
    """
    if not articles:
        return []
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    texts = [preprocess_text(article['ë‚´ìš©']) for article in articles]
    
    # TF-IDF ë²¡í„°í™”
    vectorizer = TfidfVectorizer(
        max_features=10000,  # ìµœëŒ€ ë‹¨ì–´ ìˆ˜ ì œí•œ
        min_df=2,           # ìµœì†Œ 2ê°œ ë¬¸ì„œì—ì„œ ë“±ì¥
        max_df=0.95         # ìµœëŒ€ 95% ë¬¸ì„œì—ì„œ ë“±ì¥
    )
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarity_matrix = cosine_similarity(tfidf_matrix)
    
    # ê·¸ë£¹í™”
    groups = []
    used_indices = set()
    
    # ê° ê¸°ì‚¬ì— ëŒ€í•´
    for i in range(len(articles)):
        if i in used_indices:
            continue
            
        # í˜„ì¬ ê¸°ì‚¬ì™€ ìœ ì‚¬í•œ ê¸°ì‚¬ë“¤ì„ ì°¾ìŒ
        similar_indices = [j for j in range(len(articles)) 
                         if similarity_matrix[i][j] > similarity_threshold 
                         and j not in used_indices]
        
        if similar_indices:
            # ê·¸ë£¹ ë‚´ ê¸°ì‚¬ë“¤ì„ ë°œí–‰ì¼ ìˆœìœ¼ë¡œ ì •ë ¬
            group_articles = [articles[idx] for idx in similar_indices]
            group_articles.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
            
            # ê·¸ë£¹ ë‚´ ê¸°ì‚¬ ìˆ˜ê°€ 2ê°œ ì´ìƒì¸ ê²½ìš°ì—ë§Œ ì¶”ê°€
            if len(group_articles) >= 2:
                groups.append(group_articles)
                used_indices.update(similar_indices)
    
    # ë…ë¦½ì ì¸ ê¸°ì‚¬ë“¤ë„ ì¶”ê°€ (ê·¸ë£¹ì— ì†í•˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤)
    independent_articles = [articles[i] for i in range(len(articles)) 
                          if i not in used_indices]
    
    # ë…ë¦½ì ì¸ ê¸°ì‚¬ë“¤ë„ ë°œí–‰ì¼ ìˆœìœ¼ë¡œ ì •ë ¬
    independent_articles.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
    
    # ë…ë¦½ì ì¸ ê¸°ì‚¬ë“¤ë„ ê·¸ë£¹ìœ¼ë¡œ ì¶”ê°€
    if independent_articles:
        groups.append(independent_articles)
    
    # ê° ê·¸ë£¹ì—ì„œ ëŒ€í‘œ ê¸°ì‚¬ ì„ íƒ
    final_articles = []
    for group in groups:
        if len(group) == 1:  # ë…ë¦½ì ì¸ ê¸°ì‚¬ì¸ ê²½ìš°
            final_articles.append(group[0])
        else:  # ìœ ì‚¬í•œ ê¸°ì‚¬ ê·¸ë£¹ì¸ ê²½ìš°
            # ê·¸ë£¹ ë‚´ì—ì„œ ê°€ì¥ ìµœê·¼ ê¸°ì‚¬ë¥¼ ëŒ€í‘œë¡œ ì„ íƒ
            final_articles.append(group[0])
    
    return final_articles

def deduplicate_articles(df):
    """ê¸°ì‚¬ ì¤‘ë³µì œê±°"""
    print("12. ê¸°ì‚¬ ì¤‘ë³µì œê±° ì‹œì‘...")
    
    # í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”
    grouped = df.groupby('í‚¤ì›Œë“œ')
    deduplicated_rows = []
    
    for keyword, group in grouped:
        print(f"\ní‚¤ì›Œë“œ: {keyword}")
        print(f"  - ê¸°ì‚¬ ìˆ˜: {len(group)}")
        
        if len(group) < 3:
            print("  - 3ê°œ ë¯¸ë§Œì´ë¯€ë¡œ ëª¨ë‘ í¬í•¨")
            deduplicated_rows.extend(group.to_dict('records'))
            continue
            
        # ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í™”
        group_dict = group.to_dict(orient='records')
        print(f"  - ê·¸ë£¹ ë°ì´í„° í˜•ì‹: {type(group_dict)}")
        print(f"  - ê·¸ë£¹ ë°ì´í„° ê¸¸ì´: {len(group_dict)}")
        
        similar_groups = group_similar_articles(group_dict)
        
        # ê° ìœ ì‚¬ ê·¸ë£¹ì—ì„œ ê¸°ì‚¬ ì„ íƒ
        for group_idx, group_articles in enumerate(similar_groups, 1):
            print(f"\n  ìœ ì‚¬ ê·¸ë£¹ {group_idx} ì²˜ë¦¬ ì¤‘...")
            # ê° ê·¸ë£¹ì—ì„œ ê°€ì¥ ìµœê·¼ ê¸°ì‚¬ë§Œ ì„ íƒ
            if group_articles:
                selected_article = group_articles[0]  # ê°€ì¥ ìµœê·¼ ê¸°ì‚¬ ì„ íƒ
                deduplicated_rows.append(selected_article)
                print(f"  - ì„ íƒëœ ê¸°ì‚¬: {selected_article['ì‹ ë¬¸ì‚¬']} - {selected_article['ì œëª©'][:30]}...")
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    deduplicated_df = pd.DataFrame(deduplicated_rows)
    print(f"\n13. ì¤‘ë³µì œê±° ì™„ë£Œ. ì›ë³¸: {len(df)}ê°œ, ì¤‘ë³µì œê±° í›„: {len(deduplicated_df)}ê°œ")
    return deduplicated_df

# âœ… STEP 3: ê¸°ì‚¬ ë³¸ë¬¸ ì—´ ì¶”ê°€
print("\n10. ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘...")
try:
    df = get_google_sheets_data()  # DataFrame ê°€ì ¸ì˜¤ê¸°
    df['ë³¸ë¬¸'] = df['ë§í¬'].apply(extract_article_text)
    print("11. ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ")
    
    # ì¤‘ë³µì œê±°
    df = deduplicate_articles(df)
    
except Exception as e:
    print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# âœ… STEP 4: JSON ë³€í™˜
print("14. JSON ë³€í™˜ ì‹œì‘...")
try:
    # DataFrameì„ JSONìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì»¬ëŸ¼ ì´ë¦„ì„ ì˜¬ë°”ë¥´ê²Œ ìœ ì§€í•˜ë„ë¡ ìˆ˜ì •
    json_content = df.to_json(orient='records', force_ascii=False, indent=2)
    print("15. JSON ë³€í™˜ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ JSON ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# âœ… STEP 5: GitHub Repository ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_github_repo(json_data, github_token, repo_name, file_path='news_batch.json'):
    try:
        print("16. GitHub Repository ì—…ë°ì´íŠ¸ ì‹œë„...")
        print(f"  - ì €ì¥ì†Œ: {repo_name}")
        print(f"  - íŒŒì¼ ê²½ë¡œ: {file_path}")
        print(f"  - JSON ë°ì´í„° í¬ê¸°: {len(json_data)} bytes")
        
        # GitHub API ì—”ë“œí¬ì¸íŠ¸
        url = f"https://api.github.com/repos/{repo_name}/contents/{file_path}"
        print(f"  - API URL: {url}")
        
        # ê¸°ì¡´ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        headers = {
            "Authorization": f"Bearer {github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        
        # ê¸°ì¡´ íŒŒì¼ì˜ SHA ê°€ì ¸ì˜¤ê¸°
        print("  - ê¸°ì¡´ íŒŒì¼ ì •ë³´ ìš”ì²­ ì¤‘...")
        response = requests.get(url, headers=headers)
        print(f"  - ê¸°ì¡´ íŒŒì¼ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        
        sha = None
        if response.status_code == 200:
            sha = response.json()['sha']
            print("  - ê¸°ì¡´ íŒŒì¼ SHA íšë“")
        else:
            print("  - ìƒˆ íŒŒì¼ ìƒì„±")
        
        # íŒŒì¼ ì—…ë°ì´íŠ¸
        payload = {
            "message": f"Update news data {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "content": base64.b64encode(json_data.encode()).decode(),
            "sha": sha
        }
        print("  - ì—…ë°ì´íŠ¸ ìš”ì²­ ì „ì†¡ ì¤‘...")
        
        response = requests.put(url, headers=headers, data=json.dumps(payload))
        print(f"  - ì—…ë°ì´íŠ¸ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        print(f"  - ì‘ë‹µ ë‚´ìš©: {response.text}")
        
        if response.status_code in [200, 201]:
            raw_url = response.json()['content']['download_url']
            print(f"âœ… GitHub Repository ì—…ë°ì´íŠ¸ ì„±ê³µ!")
            print(f"ğŸŒ Raw URL: {raw_url}")
            return raw_url
        else:
            print(f"âŒ GitHub Repository ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {response.status_code}")
            print(f"  - ì—ëŸ¬ ë©”ì‹œì§€: {response.text}")
            return None
    except Exception as e:
        print(f"âŒ GitHub Repository ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# âœ… STEP 6: GitHub Repositoryì— ì—…ë¡œë“œ
GITHUB_REPO = "noviachica/news_bot"  # GitHub ì €ì¥ì†Œ ì´ë¦„
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')  # GitHub Actionsì—ì„œ ì œê³µí•˜ëŠ” í† í° ì‚¬ìš©

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
print("\ní™˜ê²½ ë³€ìˆ˜ í™•ì¸:")
print(f"  - GITHUB_TOKEN ì¡´ì¬ ì—¬ë¶€: {'ìˆìŒ' if GITHUB_TOKEN else 'ì—†ìŒ'}")
if GITHUB_TOKEN:
    print(f"  - GITHUB_TOKEN ê¸¸ì´: {len(GITHUB_TOKEN)}")
    print(f"  - GITHUB_TOKEN ì‹œì‘ ë¶€ë¶„: {GITHUB_TOKEN[:10]}...")
else:
    print("  - GITHUB_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("  - GitHub Actionsì˜ secretsì— GITHUB_TOKENì´ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    print("  - secrets ì„¤ì • ë°©ë²•: https://docs.github.com/en/actions/security-guides/encrypted-secrets")

if GITHUB_TOKEN:
    print("\nGitHub ì—…ë°ì´íŠ¸ ì‹œì‘...")
    print(f"  - JSON ë°ì´í„° í¬ê¸°: {len(json_content)}")
    raw_url = update_github_repo(json_content, GITHUB_TOKEN, GITHUB_REPO)
    if raw_url:
        print(f"âœ… GitHub ì—…ë°ì´íŠ¸ ì™„ë£Œ: {raw_url}")
    else:
        print("âŒ GitHub ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
else:
    print("âš ï¸ GITHUB_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GitHub ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.") 
