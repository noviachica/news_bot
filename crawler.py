import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
import os
import json
import requests
from bs4 import BeautifulSoup
import base64
from datetime import datetime

print("1. ì‹œì‘...")

# âœ… STEP 1: Google Sheetsì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
def get_google_sheets_data():
    print("2. Google ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ì½ê¸° ì‹œë„...")
    
    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ JSON ë¬¸ìì—´ì„ ì½ì–´ì„œ íŒŒì‹±
        credentials_json = os.environ.get('GOOGLE_CREDENTIALS')
        if not credentials_json:
            print("í™˜ê²½ ë³€ìˆ˜ ë‚´ìš©:")
            print(os.environ)
            raise ValueError("GOOGLE_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        print("3. JSON ë°ì´í„° ê¸¸ì´:", len(credentials_json))
        print("4. JSON ë°ì´í„° ì‹œì‘ ë¶€ë¶„:", credentials_json[:100])  # ì²˜ìŒ 100ìë§Œ ì¶œë ¥
        
        # JSON ë¬¸ìì—´ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë””ë²„ê¹…
        with open('temp_credentials.json', 'w') as f:
            f.write(credentials_json)
        print("5. ì„ì‹œ credentials íŒŒì¼ ì €ì¥ ì™„ë£Œ")
        
        credentials_dict = json.loads(credentials_json)
        
        # ìŠ¤ì½”í”„ ì„¤ì •
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive.file",
            "https://www.googleapis.com/auth/drive"
        ]
        
        credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
        gc = gspread.authorize(credentials)
        print("6. Google Sheets ì¸ì¦ ì„±ê³µ")
        
        # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸°
        spreadsheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1HLTb59lcJQIZmaPMrJ0--hEsheyERIkCg5aBxSEFDtc/edit#gid=0')
        
        # Result ì›Œí¬ì‹œíŠ¸ ì„ íƒ
        worksheet = spreadsheet.worksheet("Result")
        print(f"7. ì„ íƒëœ ì›Œí¬ì‹œíŠ¸: {worksheet.title}")
        
        # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_values = worksheet.get_all_values()
        print(f"8. ì „ì²´ ë°ì´í„° í–‰ ìˆ˜: {len(all_values)}")
        
        # í—¤ë” í™•ì¸
        headers = all_values[0]
        print("9. í—¤ë”:", headers)
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame(all_values[1:], columns=headers)
        print(f"10. DataFrame ìƒì„± ì™„ë£Œ. í–‰ ìˆ˜: {len(df)}")
        
        return df
        
    except Exception as e:
        print(f"âŒ Google Sheets ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print(f"ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}")
        raise

# âœ… STEP 2: ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ëŸ¬ ì •ì˜
def extract_article_text(url):
    try:
        print(f"\ní¬ë¡¤ë§ ì‹œë„: {url}")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()  # HTTP ì—ëŸ¬ ì²´í¬
        print(f"HTTP ìƒíƒœ ì½”ë“œ: {res.status_code}")
        
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì— ëŒ€í•œ ì„ íƒì
        selectors = [
            'div#dic_area',  # Naver ë‰´ìŠ¤
            'div.article-body',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'article',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'div.article-content',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'div.article-text',  # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            'div#articleBody',  # ë¹„ì¦ˆë‹ˆìŠ¤í¬ìŠ¤íŠ¸
            'div#news_body_area',  # ë¹„ë°”100
            'div#article-view-content-div',  # ë‰´ìŠ¤í”„ë¼ì„
            'div.article_body',  # ë”œì‚¬ì´íŠ¸
            'div#articeBody',  # ë„¤ì´ë²„ ì—”í„°í…Œì¸ë¨¼íŠ¸
            'div.end_body',  # ë„¤ì´ë²„ ì—”í„°í…Œì¸ë¨¼íŠ¸ ëª¨ë°”ì¼
            'div#newsEndContents',  # ë„¤ì´ë²„ ìŠ¤í¬ì¸ 
            'div.article_txt',  # ì¶”ê°€ ì¼ë°˜
            'div#articleContent',  # ì¶”ê°€ ì¼ë°˜
            'div#newsContent',  # ì¶”ê°€ ì¼ë°˜
            'div.news_body',  # ì¶”ê°€ ì¼ë°˜
            'div#newsViewArea',  # ì¶”ê°€ ì¼ë°˜
            'div#content',  # ì¶”ê°€ ì¼ë°˜
        ]
        
        for selector in selectors:
            article = soup.select_one(selector)
            if article:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for tag in article.select('script, style, iframe, .reporter_area, .copyright, .promotion'):
                    tag.decompose()
                
                # í…ìŠ¤íŠ¸ ì •ì œ
                text = article.get_text(strip=True)
                text = ' '.join(text.split())  # ì—°ì†ëœ ê³µë°± ì œê±°
                
                print(f"ì„±ê³µì ìœ¼ë¡œ ë³¸ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì„ íƒì: {selector}")
                return text
        
        print("ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì„ íƒì:")
        for selector in selectors:
            elements = soup.select(selector)
            print(f"{selector}: {len(elements)}ê°œ ë°œê²¬")
            
        # ë§ˆì§€ë§‰ ì‹œë„: ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
        paragraphs = soup.find_all(['p', 'div'])
        if paragraphs:
            longest_text = max(paragraphs, key=lambda p: len(p.get_text(strip=True)))
            text = longest_text.get_text(strip=True)
            if len(text) > 100:  # ìµœì†Œ 100ì ì´ìƒì¸ ê²½ìš°ë§Œ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
                print("ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                return text
            
        return "ë³¸ë¬¸ ì—†ìŒ"

    except requests.exceptions.RequestException as e:
        print(f"ìš”ì²­ ì—ëŸ¬: {e}")
        return f"[í¬ë¡¤ë§ ì—ëŸ¬] ìš”ì²­ ì‹¤íŒ¨: {e}"
    except Exception as e:
        print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
        return f"[í¬ë¡¤ë§ ì—ëŸ¬] {e}"

# ì‹ ë¬¸ì‚¬ ê·¸ë£¹ ë° ìš°ì„ ìˆœìœ„ ì •ì˜
NEWSPAPER_GROUPS = {
    'group1': {
        'newspapers': ['ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´'],
        'priority': {'ì¡°ì„ ì¼ë³´': 1, 'ì¤‘ì•™ì¼ë³´': 2, 'ë™ì•„ì¼ë³´': 3}
    },
    'group2': {
        'newspapers': ['ê²½í–¥ì‹ ë¬¸', 'í•œê²¨ë ˆì‹ ë¬¸', 'í•œêµ­ì¼ë³´'],
        'priority': {'ê²½í–¥ì‹ ë¬¸': 1, 'í•œê²¨ë ˆì‹ ë¬¸': 2, 'í•œêµ­ì¼ë³´': 3}
    },
    'group3': {
        'newspapers': ['ë§¤ì¼ê²½ì œ', 'í•œêµ­ê²½ì œ', 'ì„œìš¸ê²½ì œ', 'ì•„ì£¼ê²½ì œ'],
        'priority': {'ë§¤ì¼ê²½ì œ': 1, 'í•œêµ­ê²½ì œ': 2, 'ì„œìš¸ê²½ì œ': 3, 'ì•„ì£¼ê²½ì œ': 4}
    }
}

def get_newspaper_group(newspaper):
    """ì‹ ë¬¸ì‚¬ê°€ ì†í•œ ê·¸ë£¹ì„ ë°˜í™˜"""
    for group_name, group_info in NEWSPAPER_GROUPS.items():
        if newspaper in group_info['newspapers']:
            return group_name
    return None

def get_newspaper_priority(newspaper):
    """ì‹ ë¬¸ì‚¬ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ë°˜í™˜"""
    for group_info in NEWSPAPER_GROUPS.values():
        if newspaper in group_info['priority']:
            return group_info['priority'][newspaper]
    return float('inf')  # ìš°ì„ ìˆœìœ„ê°€ ì—†ëŠ” ê²½ìš° ê°€ì¥ ë‚®ì€ ìš°ì„ ìˆœìœ„

def select_articles_by_length(group, threshold=0.2):
    """ê¸°ì‚¬ ê¸¸ì´ì™€ ì‹ ë¬¸ì‚¬ ìš°ì„ ìˆœìœ„ë¥¼ ê³ ë ¤í•˜ì—¬ ê¸°ì‚¬ ì„ íƒ"""
    # ê¸°ì‚¬ ê¸¸ì´ ê³„ì‚° (ë³¸ë¬¸ì˜ ê¸¸ì´)
    group['length'] = group['ë³¸ë¬¸'].str.len()
    max_length = group['length'].max()
    
    # ê¸¸ì´ ê¸°ì¤€ í•„í„°ë§ (ìµœëŒ€ ê¸¸ì´ì˜ 80% ì´ìƒì¸ ê¸°ì‚¬ë“¤)
    length_threshold = max_length * (1 - threshold)
    candidates = group[group['length'] >= length_threshold].copy()
    
    if len(candidates) == 0:
        candidates = group.copy()
    
    # ì‹ ë¬¸ì‚¬ ìš°ì„ ìˆœìœ„ ì¶”ê°€
    candidates['priority'] = candidates['ì‹ ë¬¸ì‚¬'].apply(get_newspaper_priority)
    
    # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    candidates = candidates.sort_values(['priority'])
    
    return candidates.iloc[0] if len(candidates) > 0 else None

def deduplicate_articles(df):
    """ê¸°ì‚¬ ì¤‘ë³µì œê±°"""
    print("12. ê¸°ì‚¬ ì¤‘ë³µì œê±° ì‹œì‘...")
    
    # í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”
    grouped = df.groupby('í‚¤ì›Œë“œ')
    deduplicated_rows = []
    
    for keyword, group in grouped:
        if len(group) < 3:
            # 3ê°œ ë¯¸ë§Œì´ë©´ ëª¨ë‘ í¬í•¨
            deduplicated_rows.extend(group.to_dict('records'))
            continue
            
        # ì‹ ë¬¸ì‚¬ ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ ì„ íƒ
        selected_articles = []
        used_groups = set()
        
        # ë°œí–‰ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        group = group.sort_values('ë°œí–‰ì¼', ascending=False)
        
        # ê° ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ ì„ íƒ
        for group_name, group_info in NEWSPAPER_GROUPS.items():
            if group_name in used_groups:
                continue
                
            # í•´ë‹¹ ê·¸ë£¹ì˜ ê¸°ì‚¬ë“¤ë§Œ í•„í„°ë§
            group_articles = group[group['ì‹ ë¬¸ì‚¬'].isin(group_info['newspapers'])]
            
            if len(group_articles) > 0:
                # ê¸°ì‚¬ ê¸¸ì´ì™€ ìš°ì„ ìˆœìœ„ë¥¼ ê³ ë ¤í•˜ì—¬ ì„ íƒ
                selected_article = select_articles_by_length(group_articles)
                if selected_article is not None:
                    selected_articles.append(selected_article)
                    used_groups.add(group_name)
            
            if len(selected_articles) >= 3:
                break
        
        deduplicated_rows.extend(selected_articles)
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    deduplicated_df = pd.DataFrame(deduplicated_rows)
    print(f"13. ì¤‘ë³µì œê±° ì™„ë£Œ. ì›ë³¸: {len(df)}ê°œ, ì¤‘ë³µì œê±° í›„: {len(deduplicated_df)}ê°œ")
    return deduplicated_df

# âœ… STEP 3: ê¸°ì‚¬ ë³¸ë¬¸ ì—´ ì¶”ê°€
print("\n10. ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘...")
try:
    df = get_google_sheets_data()  # DataFrame ê°€ì ¸ì˜¤ê¸°
    df['ë³¸ë¬¸'] = df['ë§í¬'].apply(extract_article_text)
    print("11. ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ")
    
    # ì¤‘ë³µì œê±°
    df = deduplicate_articles(df)
    
except Exception as e:
    print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# âœ… STEP 4: JSON ë³€í™˜
print("14. JSON ë³€í™˜ ì‹œì‘...")
try:
    json_content = df.to_json(orient='records', force_ascii=False, indent=2)
    print("15. JSON ë³€í™˜ ì™„ë£Œ")
except Exception as e:
    print(f"âŒ JSON ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit(1)

# âœ… STEP 5: GitHub Repository ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_github_repo(json_data, github_token, repo_name, file_path='news_batch.json'):
    try:
        print("16. GitHub Repository ì—…ë°ì´íŠ¸ ì‹œë„...")
        # GitHub API ì—”ë“œí¬ì¸íŠ¸
        url = f"https://api.github.com/repos/{repo_name}/contents/{file_path}"
        print(f"API URL: {url}")
        
        # ê¸°ì¡´ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        headers = {
            "Authorization": f"Bearer {github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        print("í—¤ë” ì„¤ì • ì™„ë£Œ")
        
        # ê¸°ì¡´ íŒŒì¼ì˜ SHA ê°€ì ¸ì˜¤ê¸°
        print("ê¸°ì¡´ íŒŒì¼ ì •ë³´ ìš”ì²­ ì¤‘...")
        response = requests.get(url, headers=headers)
        print(f"ê¸°ì¡´ íŒŒì¼ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        sha = None
        if response.status_code == 200:
            sha = response.json()['sha']
            print("ê¸°ì¡´ íŒŒì¼ SHA íšë“")
        else:
            print("ìƒˆ íŒŒì¼ ìƒì„±")
        
        # íŒŒì¼ ì—…ë°ì´íŠ¸
        payload = {
            "message": f"Update news data {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "content": base64.b64encode(json_data.encode()).decode(),
            "sha": sha
        }
        print("ì—…ë°ì´íŠ¸ ìš”ì²­ ì „ì†¡ ì¤‘...")
        
        response = requests.put(url, headers=headers, data=json.dumps(payload))
        print(f"ì—…ë°ì´íŠ¸ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        print(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
        
        if response.status_code in [200, 201]:
            raw_url = response.json()['content']['download_url']
            print(f"âœ… GitHub Repository ì—…ë°ì´íŠ¸ ì„±ê³µ!\nğŸŒ Raw URL: {raw_url}")
            return raw_url
        else:
            print(f"âŒ GitHub Repository ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {response.status_code}\n{response.text}")
            return None
    except Exception as e:
        print(f"âŒ GitHub Repository ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# âœ… STEP 6: GitHub Repositoryì— ì—…ë¡œë“œ
GITHUB_REPO = "noviachica/news_bot"  # GitHub ì €ì¥ì†Œ ì´ë¦„
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')  # GitHub Actionsì—ì„œ ì œê³µí•˜ëŠ” í† í° ì‚¬ìš©

if GITHUB_TOKEN:
    update_github_repo(json_content, GITHUB_TOKEN, GITHUB_REPO)
else:
    print("âš ï¸ GITHUB_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GitHub ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.") 
